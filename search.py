{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pymongo\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "from cache import SearchCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine_postgre:\n",
    "    def __init__(self, cache_size=100, cache_ttl=3600):\n",
    "        \"\"\"\n",
    "        Initializes a SearchEngine_postgre object with a specified database type and cache settings.\n",
    "\n",
    "        Args:\n",
    "        - cache_size (int): Maximum number of items to store in cache\n",
    "        - cache_ttl (int): Time-to-live (in seconds) for cached items\n",
    "        \"\"\"\n",
    "        # initialize a cache object for the search engine using the SearchCache class\n",
    "        self.cache = SearchCache(cache_size, cache_ttl)\n",
    "        self.db_conn = psycopg2.connect(database=\"postgres\", user=\"postgres\", password=\"******\", host=\"localhost\")\n",
    "        self.users_cursor = self.db_conn.cursor()\n",
    "        self.user_table = 'twitter_users_partitioned'\n",
    "    \n",
    "    ## top 10 Most popular users\n",
    "    def most_popular_users(self, n=10):\n",
    "            \"\"\"\n",
    "            Returns the n most popular Twitter users along with their tweets.\n",
    "\n",
    "            Args:\n",
    "            - n (int): Number of users to return.\n",
    "\n",
    "            Returns:\n",
    "            - list: List of the top n Twitter users, each represented as a dictionary with a 'username' key and a 'tweets' key.\n",
    "            \"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if 'most_popular_users' in self.cache:\n",
    "                print(\"Retrieving 'most popular users' from cache!\")\n",
    "                end_time = time.time()\n",
    "                print(f\"Query took {end_time - start_time:.4f} seconds\\n\")\n",
    "                return self.cache['most_popular_users']\n",
    "            else:\n",
    "                print(f\"New entry, retrieving 'most popular users' from database!\")\n",
    "\n",
    "            query = f\"\"\"\n",
    "                SELECT user_id, name, twitter_join_date, location, \n",
    "                verified, followers_count, friends_count, favourites_count\n",
    "                FROM (\n",
    "                    SELECT user_id, name, twitter_join_date, location, \n",
    "                    verified, followers_count, friends_count, favourites_count,\n",
    "                    DENSE_RANK() OVER (PARTITION BY user_id ORDER BY followers_count DESC) AS rnk\n",
    "                FROM {self.user_table}\n",
    "                        ) AS B\n",
    "                WHERE rnk = 1\n",
    "                ORDER BY followers_count DESC\n",
    "                LIMIT {n}\n",
    "                \"\"\"\n",
    "            \n",
    "            self.users_cursor.execute(query)\n",
    "            results = self.users_cursor.fetchall()\n",
    "            \n",
    "            users = []\n",
    "            for row in results:\n",
    "                user = {\n",
    "                    'user_id': row[0],\n",
    "                    'name': row[1],\n",
    "                    'twitter_join_date': row[2],\n",
    "                    'location': row[3],\n",
    "                    'verified': row[4],\n",
    "                    'followers_count': row[5],\n",
    "                    'friends_count': row[6],\n",
    "                    'favourites_count': row[7],\n",
    "                }\n",
    "                users.append(user)\n",
    "\n",
    "            users = pd.DataFrame(users)\n",
    "            self.cache['most_popular_users'] = users.to_json(orient='records')\n",
    "            self.cache.save_checkpoint()\n",
    "            end_time = time.time()\n",
    "            print(f\"Query took {end_time - start_time:.4f} seconds\\n\")\n",
    "            \n",
    "            return users  \n",
    "        \n",
    "    ## search by user_name\n",
    "\n",
    "    def search_user(self, string_user):\n",
    "            \"\"\"\n",
    "            Returns the tweets with string or hash_tag provided.\n",
    "\n",
    "            Args:\n",
    "            - string_user: # or string to match.\n",
    "\n",
    "            Returns:\n",
    "            - list: List of the tweets that match the string\n",
    "            \"\"\"\n",
    "            start_time = time.time()\n",
    "            if 'users_name'+string_user.replace(\" \",\"\") in self.cache:\n",
    "                print(\"Retrieving 'users_name' from cache!\")\n",
    "                end_time = time.time()\n",
    "                print(f\"Query took {end_time - start_time:.4f} seconds\\n\")\n",
    "                return self.cache['users_name'+string_user.replace(\" \",\"\")]\n",
    "            else:\n",
    "                print(f\"New entry, retrieving 'users_name' from database!\")\n",
    "\n",
    "\n",
    "            query = f\"\"\"\n",
    "\n",
    "                WITH ranked_users AS (\n",
    "                SELECT *, ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY followers_count DESC) AS rn\n",
    "                    FROM {self.user_table}\n",
    "                    WHERE name LIKE %s\n",
    "                )\n",
    "                SELECT name,screen_name FROM ranked_users\n",
    "                WHERE rn = 1\n",
    "                ORDER BY verified DESC, followers_count DESC;\n",
    "                \"\"\"\n",
    "            \n",
    "            # Add wildcard characters to the desired_username for searching similar usernames\n",
    "            username_pattern = '%' + string_user + '%'\n",
    "\n",
    "            self.users_cursor.execute(query, (username_pattern,))\n",
    "\n",
    "            results = self.users_cursor.fetchall()\n",
    "            \n",
    "            users = []\n",
    "            for row in results:\n",
    "                user = {\n",
    "                    'name': row[0],\n",
    "                    'screen_name': row[1]\n",
    "                    }\n",
    "                users.append(user)\n",
    "\n",
    "            users = pd.DataFrame(users)\n",
    "            if users.shape[0] == 0:\n",
    "                users = pd.DataFrame([\"No User found\"], index= [string_user])\n",
    "                print(\"No User found\")\n",
    "            self.cache['users_name'+string_user.replace(\" \",\"\")] = users.to_json(orient='records')\n",
    "            self.cache.save_checkpoint()\n",
    "            end_time = time.time()\n",
    "            print(f\"Query took {end_time - start_time:.4f} seconds\\n\")\n",
    "            \n",
    "            return users  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SearchEngine object with cache size of 50 and cache TTL of 3600 seconds\n",
    "search_engine_postgre = SearchEngine_postgre(cache_size=50, cache_ttl=3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the Most Popular Users and updating the Checkpoint\n",
    "import time\n",
    "starttime = time.perf_counter()\n",
    "mpu = search_engine_postgre.most_popular_users()\n",
    "print(time.perf_counter() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing Most Popular Users\n",
    "mpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreving the tweets from cache and checking the time taken to retrev data from cache\n",
    "import time\n",
    "starttime = time.perf_counter()\n",
    "mpu_cache = search_engine_postgre.most_popular_users()\n",
    "print(time.perf_counter() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing Most Popular Users from cache\n",
    "mpu_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.perf_counter()\n",
    "user_search = search_engine_postgre.search_user(\"Nikhitha\")\n",
    "print(time.perf_counter() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.perf_counter()\n",
    "user_search_cache = search_engine_postgre.search_user(\"Nikhitha\")\n",
    "print(time.perf_counter() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_search_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client = pymongo.MongoClient('mongodb+srv://priyankanagasuri:*********@cluster1.dfkwly1.mongodb.net/')\n",
    "db = db_client.get_database(\"twitter_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine_mongodb:\n",
    "    def __init__(self, cache_size=100, cache_ttl=3600):\n",
    "        \"\"\"\n",
    "        Initializes a SearchEngine object with a specified database type and cache settings.\n",
    "\n",
    "        Args:\n",
    "        - cache_size (int): Maximum number of items to store in cache\n",
    "        - cache_ttl (int): Time-to-live (in seconds) for cached items\n",
    "        \"\"\"\n",
    "        # initialize a cache object for the search engine using the SearchCache class\n",
    "        self.cache = SearchCache(cache_size, cache_ttl)\n",
    "        self.db_client = pymongo.MongoClient('mongodb+srv://priyankanagasuri:littlegirl369@cluster1.dfkwly1.mongodb.net/')\n",
    "        self.tweets_collection = self.db_client['twitter_db']['tweets_final']\n",
    "        \n",
    "   ## Search by string\n",
    "    def search_by_string(self, string_to_match):\n",
    "            \"\"\"\n",
    "            Returns the tweets with string provided.\n",
    "\n",
    "            Args:\n",
    "            - string_to_match: string to match.\n",
    "\n",
    "            Returns:\n",
    "            - list: List of the tweets that match the string\n",
    "            \"\"\"\n",
    "            start_time = time.time()            \n",
    "            if 'string_match_cache' + string_to_match in self.cache:\n",
    "                print(\"Retrieving tweets with \" + string_to_match +  \" from cache!\")\n",
    "                end_time = time.time()\n",
    "                print(f\"Query took {end_time - start_time:.4f} seconds\\n\")\n",
    "                return self.cache['string_match_cache' + string_to_match]\n",
    "            else:\n",
    "                print(f\"New entry, retrieving tweets \" + string_to_match + \" from database!\")\n",
    "\n",
    "            results = list(self.tweets_collection.aggregate([\n",
    "                {\n",
    "                \"$search\": {\n",
    "                \"index\": \"search_tweets\",\n",
    "                \"text\": {\n",
    "                \"query\": string_to_match,\n",
    "                \"path\": \"tweet\"\n",
    "                }\n",
    "                }\n",
    "                }   \n",
    "                ]))\n",
    "\n",
    "            users = []\n",
    "            for row in results:\n",
    "                user = {\n",
    "                    'Account_name': row[\"Account_Name\"],\n",
    "                    'text': row[\"tweet\"],\n",
    "                    'date': row[\"Time_stamp\"],\n",
    "                    'Likes': row[\"Likes\"]\n",
    "                    }\n",
    "                users.append(user)\n",
    "\n",
    "            users = pd.DataFrame(users)\n",
    "            cache_name = \"string_match_cache\" + string_to_match\n",
    "            self.cache[cache_name] = users.to_json(orient='records')\n",
    "            self.cache.save_checkpoint()\n",
    "            end_time = time.time()\n",
    "            print(f\"Query took {end_time - start_time:.4f} seconds\\n\")\n",
    "            \n",
    "            return users  \n",
    "\n",
    "   ## Search by Hastag\n",
    "    def search_by_hashtag(self, hashtag_to_match):\n",
    "            \n",
    "            \"\"\"\n",
    "            Returns the tweets with hashtag provided.\n",
    "\n",
    "            Args:\n",
    "            - hashtag_to_match: string to match.\n",
    "\n",
    "            Returns:\n",
    "            - list: List of the tweets that match the hashtag\n",
    "            \"\"\"\n",
    "            \n",
    "            start_time = time.time()\n",
    "            if 'hashtag_match_cache' + hashtag_to_match in self.cache:\n",
    "                print(\"Retrieving tweets with  hashtags \" + hashtag_to_match +\" from cache!\")\n",
    "                end_time = time.time()\n",
    "                print(f\"Query took {end_time - start_time:.4f} seconds\\n\")\n",
    "                return self.cache['hashtag_match_cache' + hashtag_to_match]\n",
    "            else:\n",
    "                print(f\"New entry, retrieving tweets \" + hashtag_to_match + \" from database!\")\n",
    "\n",
    "            results = list(self.tweets_collection.aggregate([\n",
    "                {\n",
    "                \"$search\": {\n",
    "                \"index\": \"search_tweets\",\n",
    "                \"text\": {\n",
    "                \"query\": hashtag_to_match,\n",
    "                \"path\": \"hashtags\"\n",
    "                }\n",
    "                }\n",
    "                }\n",
    "                ]))\n",
    "\n",
    "            users = []\n",
    "            for row in results:\n",
    "                user = {\n",
    "                    'Account_name': row[\"Account_Name\"],\n",
    "                    'text': row[\"tweet\"],\n",
    "                    'date': row[\"Time_stamp\"],\n",
    "                    'Likes': row[\"Likes\"]\n",
    "                    }\n",
    "                users.append(user)\n",
    "\n",
    "            users = pd.DataFrame(users)\n",
    "            cache_name = \"hashtag_match_cache\" + hashtag_to_match\n",
    "            self.cache[cache_name] = users.to_json(orient='records')\n",
    "            self.cache.save_checkpoint()\n",
    "            end_time = time.time()\n",
    "            print(f\"Query took {end_time - start_time:.4f} seconds\\n\")\n",
    "\n",
    "            return users\n",
    "\n",
    "   ## Top 10 Hastags\n",
    "    def get_top_hashtags(self,n):\n",
    "\n",
    "        \"\"\"\n",
    "        Returns the tweets with hashtag provided.\n",
    "\n",
    "        Args:\n",
    "        - n: No of hashtags to return\n",
    "\n",
    "        Returns:\n",
    "        - list: List of the top hashtags\n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        if 'top_hashtags_' + str(n) in self.cache:\n",
    "            print(\"Retrieving top \" +str(n)+ \"  hashtags from cache!\")\n",
    "            end_time = time.time()\n",
    "            print(f\"Query took {end_time - start_time:.4f} seconds\\n\")\n",
    "            return self.cache['top_hashtags_' + str(n)]\n",
    "        else:\n",
    "            print(f\"New entry, retrieving top \" + str(n) +\" hashtags from database!\")\n",
    "\n",
    "\n",
    "            # Pipeline to aggregate and retrieve top 10 hashtags based on likes_count\n",
    "            pipeline = [\n",
    "                # Unwind the hashtags array\n",
    "                {\"$unwind\": \"$hashtags\"},\n",
    "                # Group by hashtag and count occurrences\n",
    "                {\"$group\": {\"_id\": \"$hashtags\", \"count\": {\"$sum\": 1}}},\n",
    "                # Sort by count in descending order\n",
    "                {\"$sort\": {\"count\": -1}},\n",
    "                # Limit to top 10 hashtags\n",
    "                {\"$limit\": n}\n",
    "            ]\n",
    "\n",
    "            # Execute the aggregation pipeline\n",
    "            results = self.tweets_collection.aggregate(pipeline)\n",
    "            \n",
    "            users = pd.DataFrame(results._CommandCursor__data)\n",
    "            users.columns = [\"hashtag\",\"count\"]    \n",
    "            cache_name = \"top_hashtags_\" + str(n)\n",
    "            self.cache[cache_name] = users.to_json(orient='records')\n",
    "            self.cache.save_checkpoint()\n",
    "            end_time = time.time()\n",
    "            print(f\"Query took {end_time - start_time:.4f} seconds\\n\")\n",
    "\n",
    "        return users\n",
    "\n",
    "   ## Top 20 Tweets\n",
    "    def top_tweets(self,n):\n",
    "\n",
    "        start_time = time.time()\n",
    "        if 'top_tweets_' + str(n) in self.cache:\n",
    "            print(\"Retrieving top \" +str(n)+ \"  tweets from cache!\")\n",
    "            end_time = time.time()\n",
    "            print(f\"Query took {end_time - start_time:.4f} seconds\\n\")\n",
    "            return self.cache['top_tweets_' + str(n)]\n",
    "        else:\n",
    "            print(f\"New entry, retrieving top \" + str(n) +\" tweets from database!\")\n",
    "\n",
    "        results =self.tweets_collection.find().sort(\"retweets_count\",-1).limit(n)\n",
    "        \n",
    "\n",
    "        users = pd.DataFrame(results)\n",
    "        users = users[[\"Time_stamp\",\"Account_Name\",\"tweet\",\"retweets_count\"]]    \n",
    "        cache_name = \"top_tweets_\" + str(n)\n",
    "        self.cache[cache_name] = users.to_json(orient='records')\n",
    "        self.cache.save_checkpoint()\n",
    "        end_time = time.time()\n",
    "        print(f\"Query took {end_time - start_time:.4f} seconds\\n\")\n",
    "\n",
    "        return users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SearchEngine object with cache size of 50 and cache TTL of 3600 seconds\n",
    "search_engine2 = SearchEngine_mongodb(cache_size=50, cache_ttl=3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the string_search tweets and updating the Checkpoint\n",
    "starttime = time.perf_counter()\n",
    "string_search = search_engine2.search_by_string(  \"ram\"  )\n",
    "print(time.perf_counter() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the String_search tweets\n",
    "string_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreving the tweets from cache and checking the time taken to retrev data from cache\n",
    "import time\n",
    "starttime = time.perf_counter()\n",
    "cache_string_search = search_engine2.search_by_string(\"ram\")\n",
    "print(time.perf_counter() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the string_search tweets from cache\n",
    "cache_string_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the hashtag_search tweets and updating the Checkpoint\n",
    "starttime = time.perf_counter()\n",
    "hashtag_search = search_engine2.search_by_hashtag(\"COVID19InTurkeyPrisons\")\n",
    "print(time.perf_counter() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the Hashtag_search tweets\n",
    "hashtag_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreving the tweets from cache and checking the time taken to retrev data from cache\n",
    "starttime = time.perf_counter()\n",
    "hashtag_search_cache = search_engine2.search_by_hashtag(\"COVID19InTurkeyPrisons\")\n",
    "print(time.perf_counter() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the Hastage_search tweets from cache\n",
    "hashtag_search_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the top_10_hastags tweets and updating the Checkpoint\n",
    "starttime = time.perf_counter()\n",
    "top_10_hashtags = search_engine2.get_top_hashtags(10)\n",
    "print(time.perf_counter() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the top_10_hastages \n",
    "top_10_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreving the tweets from cache and checking the time taken to retrev data from cache\n",
    "starttime = time.perf_counter()\n",
    "top_10_hashtags_cache = search_engine2.get_top_hashtags(10)\n",
    "print(time.perf_counter() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the Hastage_search tweets from cache\n",
    "top_10_hashtags_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the top_tweets tweets \n",
    "search_engine2.top_tweets(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreving the tweets from cache and checking the time taken to retrev data from cache\n",
    "starttime = time.perf_counter()\n",
    "top_tweets_cache = search_engine2.top_tweets(20)\n",
    "print(time.perf_counter() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the top_tweets tweets from cache\n",
    "top_tweets_cache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
